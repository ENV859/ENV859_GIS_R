---
title: "Intro to R, GIS R"
author: "Matthew Ross"
date: "November 6, 2016"
output: 
  html_document:
    toc: true
    toc_depth: 3
---
#Intro to R

Before we jump into geospatial analysis in R, let's review general properties of R itself. 


##What is R? 

R is a versatile statistical programming language, but it can also be used for geospatial analysis, making interactive data visualizations, and, with the help of RStudio, it can even be used to write papers. 

##Why use R for geospatial analysis?

- Free, and open source
- Packages make it easy to do a broad array of analyses from genomics to geodesy
- Scripting makes work inherently reproducible
- RMarkdown is a wonderful way to share and comment code
- Github integration makes it easy to collaborate on projects
- Large and intuitive plotting library 
- I learned it before I learned python

##How do I use R?

R takes hours to learnthe basics and a lifetime to master, like any language. But let's get started. 

Like Python R is a object-oriented language, which basically means that you can assign objects to have specific features. For example, R can think of the string: "10/31/1987" as simply a string of characters or as a factor or most usefully has a Date object. For example: If R thinks of "10/31/1987" it will have specific attributes associated with that object, meaning you can subtract date objects or add them together. To work with date objects I like to use a package called lubridate. Like here:

###Object example

```{r objects}
# R sees the following as a vector (sequence) of separate character objects.
dates <- c('10/31/1987','10/31/1988')
#Str tells us the structure of the object
str(dates)
# chr means character or string in python terms
#Let's turn this into a Date object 
#First we need to load a library
library('lubridate')
dts <- mdy(dates)
#Now let's check the structure of this new object:
str(dts)
#It is now a date object and we can do interesting stuff with it. 
#Like add a day
dts + 1
#Or expand the date to fill out the year
dts.yr <- seq(dts[1],dts[2],length=365)
head(dts.yr)
```


###Data Frames

So that is a simple example of how R works. A lot of the time when working in R you will use a object called a data frame, which is essentially like a sheet in excel. For example, here is a data sheet of total surface mining extent across a few Appalachian states. 


```{r dataframes}
#read in my data in the data folder, I don't want R to call strings factors, hence the second command
mining <- read.csv('data/CumeMine.csv',stringsAsFactors=F)
#What does this data look like?
head(mining)
#Headers look like state names with some empty columns that have notes in them.
names(mining)
#Second column looks like total areas by state. #Let's extract that column and put it in a different dataset
# I can pull this out by grabbing the first row and columns 2-5
tote.areas<- mining[1,2:5]
#check
str(tote.areas)
#oops those are characters not numbers.
as.numeric(tote.areas)
#Oops that doesn't work? I'm going to have to do some tricks to remove the commas. 
tote.area.no.comma <- gsub(pattern=',',replacement='',x=tote.areas)
#Now I can convert it to a numeric. 
tote.area.num <- as.numeric(tote.area.no.comma)
#But we lost the state names, let's bind it back together.
total.areas <- data.frame(state=names(mining)[2:5],area=tote.area.num,stringsAsFactors = F)
total.areas

#Now let's fix the original data frame using sine package called tidyr dplyr and magrittr
library(dplyr) # A great library of data fitlering tools
library(tidyr) # Easiest way to make data tidy
library(magrittr) # Adds pipe functionality (makes code less verbose, think arrows in model builder)

#Reread in data and skip first row this time. And keep only first five columns
mine.dat <- read.csv('data/CumeMine.csv',stringsAsFactors=F,skip=2,header=F)[1:5]
#Rename columns
names(mine.dat) <- c('year','kentucky','tennessee','virginia','west.virginia')
#Convert to a tidy dataset by 'gathering' data
annual.mining <- gather(mine.dat,key=state,value=mining,-year) %>% 
  select(state,year,mining) %>% arrange(year) 
#Tidy data!
head(annual.mining)
```

###Plotting
With our tidy dataset in hand, plotting is easy. We can look at data in so many different ways. 

How do mining rates differ over time and between states? Let's use ggplot2 and find out!
```{r glines}

library(ggplot2)

glines <- ggplot(annual.mining,aes(x=year,y=mining,color=state)) + geom_line()
glines
```

What about cumulative mining extent?
```{r gstacks}
gstack <- ggplot(annual.mining,aes(x=year,y=mining,fill=state)) + geom_area(position='stack')
gstack

#I don't like that order, let's reorder the stacking position by using factors
annual.mining$States <- factor(annual.mining$state,levels=c('tennessee','virginia','west.virginia','kentucky'))

#Pipes too!
gstack1 <- arrange(annual.mining,States) %>% ggplot(aes(x=year,y=mining,fill=States)) + geom_area(position='stack')
gstack1
```


What about the correlation between mining in West Virginia and Kentucky?

This time our data is not in the right structure to immediately look at this correlation but that is easy to fix using the command "spread" from *tidyr* package
```{r lms}
#First remove factor version of states
spread.mining <- select(annual.mining,-States) %>% 
  spread(key=state,value=mining)
#Back to the original structure.
head(spread.mining)

ky.wv <- ggplot(spread.mining,aes(x=kentucky,y=west.virginia)) + geom_point()

# We can even easily add a linear model to this data.
ky.wv1 <- ggplot(spread.mining,aes(x=kentucky,y=west.virginia,label=year)) + 
    geom_smooth(method='lm') +
  geom_point() 


# And print the model summary.
mine.model <- lm(west.virginia~kentucky,data=spread.mining)
summary(mine.model)

#Looks like for every 1 square meter mined in KY there is 0.77 m2 mined in WV. 

```

We can easily add interactivity to these plots using plotly library
```{r ggplotly}
library(plotly)
ggplotly(ky.wv1)

```


###  Assignment 1
Now you have enough knowledge to create your own plots. Let's try to plot cumulative mining as a percentage of total area. 

```{r assignment1}
#First we need to join the total area data with the annual mining extent data.
p.mining <- left_join(annual.mining,total.areas,by='state') %>% 
  #Then add a column dividing mining extent by total area
  mutate(percent = mining/area)
head(p.mining)
#Now you make a plot here!

```


#Geospatial R
Ok, now you have a basic understanding of how to use R let's make it spatial!

For this example I am going to trim some data from my research and move it into the directory for the class. But first I'm going to load in our key libraries

##Key Libraries
```{r geolibraries}
library(rgdal) # For reprojection and raster package
library(raster) #For playing with raster data
library(sp) #For shapefiles
library(rgeos) #For area estimation. 

```

A key warning pops up here saying that "select" is masked from dplyr, meaning when we use the select command R will default to using the raster version of select, not the dplyr version. So if we want to use dplyr version we have to do a python like thing: "dplyr::select"


So now we have our libraries to trim by 2GB dataset into something smaller. This code chunk will not work, but I'm leaving it here for posterity. And to show you some RMarkdown tricks!

By saying "eval=false" in the code chunk naming, this code will not run when you stitch this html. 

##Data Trim 
```{r dattrim,eval=F,echo=F,include=F}
#Read in our study watershed outline. 
setwd("~/Dropbox/Shared Science/NSF_MTM_All/MTM_GIS")
#readOGR(folder,shapefilename)
mud.river <- readOGR('Shapefiles/watershedoutlines','MR14_ned')
stanley <- readOGR('Shapefiles/watershedoutlines/StanleyFork','stanleyfork_lidar')

#Get raster directory
rast.dir <- "~/Dropbox/Shared Science/NSF_MTM_All/MTM_GIS/Pericak_Summer16/Data/outputProducts/yearlyMiningTIFs"
#Get mining extent by year
rast.years <- list.files(rast.dir)
#get full directories
rast.files <- paste(rast.dir,rast.years,sep='/')


#Read in first raster in list
r <- raster(rast.files[1])
#Transform shapefile to same projection as raster
mud.match <- spTransform(mud.river,projection(r))
#crop raster to match only study extent.
mud.r <- crop(r,mud.match)
#Mask out values outside of the watershed. 
mud.r <- mask(mud.r,mud.match)


#Store data in local directory. 
stor.dir <- "~/Dropbox/Teaching/ENV859_GIS_R/data/rasters"

#Setup a loop to do this for every file in the folder
for(i in 1:length(rast.files)){
  #Read in first raster
  r <- raster(rast.files[i])
  #crop raster to match only study extent.
  mud.r <- crop(r,mud.match)
  #Mask out values outside of the watershed. 
  mud.r.mask <- mask(mud.r,mud.match)
  #Setup a storage file place and name. 
  stor.file <- paste(stor.dir,rast.years[i],sep='/')
  #Write out clipped raster. 
  writeRaster(mud.r.mask,filename=stor.file,overwrite=T)
}

#Also need to save the full shapefiles. 
setwd("~/Dropbox/Teaching/ENV859_GIS_R/data/shapefiles")


writeOGR(mud.match,'Watersheds',layer='mud',driver='ESRI Shapefile')


writeOGR(stanley,'Watersheds',layer='stanley',driver='ESRI Shapefile',overwrite_layer=T)

```

##Shapefiles

For this exercise we will be using watershed outlines from two of my study watersheds. First we need to read these shapefiles in and then we can play around with them. 

To read in the data we will use readOGR from the rgdal package, like this readOGR(*folder*, shapefile)

```{r rgdal}

mud.river <- readOGR('data/shapefiles/Watersheds','mud')
stanley.fork <- readOGR('data/shapefiles/Watersheds','stanley')
#Print information on the shapefile
mud.river
stanley.fork

plot(mud.river)
plot(stanley.fork,add=T)

```

Ok so now we have two shapefile and we can plot them, though the plot is pretty boring. We will fix that later, but for context here is the general area we are looking at. 

(echo = false hides this code.)
```{r leafletsample, echo=F}
library(leaflet)
leaflet() %>% addProviderTiles('Esri.WorldImagery') %>% 
    setView(lng = -81.93603515625, lat = 38.1046650598288, zoom = 14)

```





First we need to fix the fact that stanley fork, which is a watershed inside of the mud river watershed, is not plotting. Look at the coord output to see why.

```{r reproject}
#Our shapefiles don't have matching projections, but that is easy to fix!
stanley.match <- spTransform(stanley.fork, projection(mud.river))

plot(mud.river)
plot(stanley.match,add=T,col='blue')

```

##Rasters

Now we have some watershed outlines and maybe we want to figure out how much mining there was each year in these watersheds. In the raster folder we have annual mining extent for the mud river as rasters (as .tifs). So let's play with the raster library and see what this data looks like. 

```{r rasters}
# Let's see what mining looks like from 1999
mine.99 <- raster('data/rasters/mining_1999_1014.tif')
#Plot the data. 
plot(mine.99)
plot(mud.river,add=T)

```

### Crop, masking, and trimming
Let's look just at stanley fork. 

```{r stancrop}
stan.mine <- crop(mine.99,stanley.match)
plot(stan.mine, main='Cropped to extent only')
plot(stanley.match,add=T)

# But that only cropped the data to the extent of stanley fork watershed. 
mask(stan.mine,stanley.match) %>% 
  plot(., main='Cropped to watershed outline') # The dot just retrieves the previous object fed from the pipe!
plot(stanley.match,add=T)

#We can trim this raster even more though witht the trim command
mask(stan.mine,stanley.match) %>% 
  trim(.) %>%
  plot(., main='Cropped and trimmed to watershed outline')
plot(stanley.match,add=T)


```

###Summarising raster data
What information can we extract from our raster data? 
```{r summarydata}
#What is the area of this watershed?
stan.area <- area(stanley.match) # this is in m2, km2 makes more sense
stan.area <- area(stanley.match)/(1000*1000)

# Mask stan.mine again.
stan.shed <- mask(stan.mine,stanley.match)
#How many cells are in stan.shed? 
ncell(stan.shed)
#But this includes a bunch of NAs so we can't use it to calculate area. 

#Let's set NAs to 0
stan.shed[is.na(stan.shed)] <- 0
#This works because each cell is actually the area. 900m2 so we can just sum up
#The non-zero cells. 
tote99 <- sum(as.matrix(stan.shed))/(1000*1000)

tote99/stan.area

```

###Extracting data from rasters 
Wow 83% of the watershed was undergoing mining in 1999. What about % active mining for every year of the watershed? Here we can use a loop to answer this question. 

```{r activeMining}
library(stringr)

#I want to make sure I get the years right, so I will use stringr to make a vector that
# lists the year from the .tif raster name. 
rast.name <- list.files('data/rasters')
years <- str_split_fixed(rast.name,'_',3) #Splits string into 3 column data frame
head(years)
#All we want is the second column
years <- as.numeric(years[,2])
#Now let's setup a director to grab all rasters
rast.files <- paste('data/rasters',rast.name,sep='/')

#Now we need to setup an empty data frame to hold the results of our loop
stan.ann.mine <- data.frame(year=numeric(),mining=numeric(),pmining=numeric())

#Practice loop, good idea when building a loop that may take some time to run
i <- 1

#Read in our raster data 
mining.yr <- raster(rast.files[i])
#Trim it to stanely fork by first cropping it and then masking it
stan.mining.yr <- crop(mining.yr, stanley.match) %>% mask(.,stanley.match)
#Let's set NAs to 0
stan.mining.yr[is.na(stan.mining.yr)] <- 0
#Sum cells
tote.yr <- sum(as.matrix(stan.mining.yr))/(1000*1000) #km2
#Get percent.
tote.p <- tote.yr/stan.area
#Put data into data frame (i = the row where data will go)
#c concatenates data together
stan.ann.mine[i,] <- c(years[i],tote.yr,tote.p)

#looks good.
stan.ann.mine

#Now all we have to do is put this into a loop (normally I would delete the practice part, but leaving it here to show how I construct loops)
for(i in 1:length(rast.files)){
  #Read in our raster data 
  mining.yr <- raster(rast.files[i])
  #Trim it to stanely fork by first cropping it and then masking it
  stan.mining.yr <- crop(mining.yr, stanley.match) %>% mask(.,stanley.match)
  #Let's set NAs to 0
  stan.mining.yr[is.na(stan.mining.yr)] <- 0
  #Sum cells
  tote.yr <- sum(as.matrix(stan.mining.yr))/(1000*1000) #km2
  #Get percent.
  tote.p <- tote.yr/stan.area
  #Put data into data frame (i = the row where data will go)
  #c concatenates data together
  stan.ann.mine[i,] <- as.numeric(c(years[i],tote.yr,tote.p))
}

#Same thing but in the apply, function format that is almost always 
# Faster, but less intuitive to code (at least initially. )
# mine.annualizer <- function(x){
#   mining.yr <- raster(x)
#   #Trim it to stanely fork by first cropping it and then masking it
#   stan.mining.yr <- crop(mining.yr, stanley.match) %>% mask(.,stanley.match)
#   #Let's set NAs to 0
#   stan.mining.yr[is.na(stan.mining.yr)] <- 0
#   #Sum cells
#   tote.yr <- sum(as.matrix(stan.mining.yr))/(1000*1000) #km2
#   #Get percent.
#   tote.p <- tote.yr/stan.area
#   #Put data into data frame (i = the row where data will go)
#   #c concatenates data together
#   dat <- cbind(tote.yr,tote.p)
#   return(dat)
# }
# mines <- sapply(rast.files,mine.annualizer)
# 

```


Now let's plot our data.
```{r}

ggplot(stan.ann.mine,aes(x=year,y=mining)) + geom_point() +
  ylab(expression(paste('Active Mining (',km^2,')',sep='')))


```


###Assignment 2

Make the same plot with mining as percent mining. Then make it interactive!
```{r assignment2}

```

###RasterStacks
This data doesn't really show us cumulative mining, just active mining extent for any given year, but getting cumulative mining extent is easy with rasterStacks!

```{r RasterStacks}

#Read in all of the annual mining extent years into a raster stack trim to stanley
stan.stack <- stack(rast.files) %>% 
  crop(.,stanley.match) %>%
  mask(.,stanley.match)
  
# Turn NAs into 0s
stan.stack[is.na(stan.stack)] <- 0

#sum every cell in the raster stack
cume.mine <- sum(stan.stack)
#Set non-0 values to 900 (area)
cume.mine[cume.mine != 0] <- 900
tote.mine.all <- sum(as.matrix(cume.mine))/(1000*1000)
tote.mine.all/stan.area
#animate(stan.stack, n=2)


```

Total cumulative mining is only 2% more than the miing in 1999. 

#Fin



